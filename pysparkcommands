pyspark --packages com.databricks:spark-csv_2.11:1.5.0

typedf = sqlContext.read.format('com.databricks.spark.csv').options(header = False, inferschema='true').load('types.csv')
    
header = ['id','CMPLNT_NUM','CMPLNT_FR_DT','CMPLNT_FR_TM','CMPLNT_TO_DT',
        'CMPLNT_TO_TM','RPT_DT','KY_CD','OFNS_DESC','PD_CD','PD_DESC',
        'CRM_ATPT_CPTD_CD','LAW_CAT_CD','JURIS_DESC','BORO_NM',
        'ADDR_PCT_CD','LOC_OF_OCCUR_DESC','PREM_TYP_DESC','PARKS_NM',
        'HADEVELOPT','X_COORD_CD','Y_COORD_CD','Latitude','Longitude','Lat_Lon']

for i in range(25):
   typedf = typedf.withColumnRenamed("C" + str(i), header[i])

df = sqlContext.read.format('com.databricks.spark.csv').options(header=True,inferschema='true').load('NYPD_Complaint_Data_Historic.csv')

typedf.groupby('CMPLNT_TO_DT').count().show()

sqlContext.registerDataFrameAsTable(df,'crimedata')
sqlContext.registerDataFrameAsTable(typedf,'crimetype')

sqlContext.sql("SELECT crimedata.CMPLNT_TO_DT FROM crimetype inner join crimedata on crimedata.CMPLNT_NUM = crimetype.id where crimetype.CMPLNT_TO_DT = 'str'").show(10)

sqlContext.sql("SELECT count(*) FROM crimetype inner join crimedata on crimedata.CMPLNT_NUM = crimetype.id where crimetype.CMPLNT_TO_DT = 'str'").show(10)


